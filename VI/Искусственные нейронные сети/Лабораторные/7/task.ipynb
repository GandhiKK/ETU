{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "pythonjvsc74a57bd0b9cd369297badecef033754457a409e1f5bc5a1dd91d77fefd2fd9bfe0bf4093",
      "display_name": "Python 3.8.7 64-bit ('.pyDev': venv)"
    },
    "metadata": {
      "interpreter": {
        "hash": "b9cd369297badecef033754457a409e1f5bc5a1dd91d77fefd2fd9bfe0bf4093"
      }
    },
    "colab": {
      "name": "task.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python",
      "version": "3.8.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMwVls8xjjJ7"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "from matplotlib import gridspec\n",
        "import re\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPool1D, GRU, Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwTsd13TjjKF",
        "outputId": "8fe15b21-a730-4606-95c3-993279dcac4d"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "d:\\Extra\\pyDev\\.pyDev\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "d:\\Extra\\pyDev\\.pyDev\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLCgO7YljjKK"
      },
      "source": [
        "X_test = data[:10000]\n",
        "Y_test = targets[:10000]\n",
        "X_train = data[10000:]\n",
        "Y_train = targets[10000:]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBJDhirDaJM7"
      },
      "source": [
        "max_review_length = 500 # макс. длина текста\n",
        "embedding_vector_length = 32 # 32-мерное векторное представление\n",
        "top_words = 10000 # количество слов, рассматриваемых как признаки"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTVc09kXzsQR"
      },
      "source": [
        "print(X_train[0])\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "print(X_train[0])\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 13, 104, 14, 9, 31, 7, 4, 4343, 7, 4, 3776, 3394, 2, 495, 103, 141, 87, 2048, 17, 76, 2, 44, 164, 525, 13, 197, 14, 16, 338, 4, 177, 16, 6118, 5253, 2, 2, 2, 21, 61, 1126, 2, 16, 15, 36, 4621, 19, 4, 2, 157, 5, 605, 46, 49, 7, 4, 297, 8, 276, 11, 4, 621, 837, 844, 10, 10, 25, 43, 92, 81, 2282, 5, 95, 947, 19, 4, 297, 806, 21, 15, 9, 43, 355, 13, 119, 49, 3636, 6951, 43, 40, 4, 375, 415, 21, 2, 92, 947, 19, 4, 2282, 1771, 14, 5, 106, 2, 1151, 48, 25, 181, 8, 67, 6, 530, 9089, 1253, 7, 4, 2]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    1   13  104   14    9   31    7    4 4343    7\n",
            "    4 3776 3394    2  495  103  141   87 2048   17   76    2   44  164\n",
            "  525   13  197   14   16  338    4  177   16 6118 5253    2    2    2\n",
            "   21   61 1126    2   16   15   36 4621   19    4    2  157    5  605\n",
            "   46   49    7    4  297    8  276   11    4  621  837  844   10   10\n",
            "   25   43   92   81 2282    5   95  947   19    4  297  806   21   15\n",
            "    9   43  355   13  119   49 3636 6951   43   40    4  375  415   21\n",
            "    2   92  947   19    4 2282 1771   14    5  106    2 1151   48   25\n",
            "  181    8   67    6  530 9089 1253    7    4    2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycRHwGt10vBH"
      },
      "source": [
        "model_a = Sequential([\n",
        "    Embedding(top_words, embedding_vector_length, input_length=max_review_length),\n",
        "    Conv1D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPool1D(2),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(128, 3, padding='same', activation='relu'),\n",
        "    MaxPool1D(2),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64, return_sequences=True, dropout=0.2),\n",
        "    LSTM(32),\n",
        "    Dense(1, activation='sigmoid')])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-9lbeO91Mh1"
      },
      "source": [
        "model_b = Sequential([\n",
        "    Embedding(top_words, embedding_vector_length, input_length=max_review_length),\n",
        "    LSTM(100, return_sequences=True, dropout=0.3),\n",
        "    LSTM(50, dropout=0.3),\n",
        "    Dense(1, activation='sigmoid')])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozVAP2AwvM42"
      },
      "source": [
        "model_c = Sequential([\n",
        "    Embedding(top_words, embedding_vector_length, input_length=max_review_length),\n",
        "    Conv1D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPool1D(2),\n",
        "    Dropout(0.5),\n",
        "    Conv1D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPool1D(2),\n",
        "    Dropout(0.4),\n",
        "    Flatten(),\n",
        "    Dense(128),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W2LfgUtcDe6"
      },
      "source": [
        "ans_len = 3\n",
        "train_size, test_size = len(X_train) // ans_len, len(X_test) // ans_len"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpvac2-u2AQz",
        "outputId": "d00e82e0-89cd-46f5-88df-dcbb99401a68"
      },
      "source": [
        "members = [model_a, model_b, model_c]\n",
        "epochs = [2, 2, 2]\n",
        "for i, mod in enumerate(members):\n",
        "    x_train = X_train[i * train_size : (i + 1) * train_size]\n",
        "    y_train = Y_train[i * train_size : (i + 1) * train_size]\n",
        "    x_test = X_test[i * test_size : (i + 1) * test_size]\n",
        "    y_test = Y_test[i * test_size : (i + 1) * test_size]\n",
        "    mod.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    mod.fit(x_train, y_train, validation_split=0.1, epochs=epochs[i], batch_size=64)\n",
        "    scores = mod.evaluate(x_test, y_test, verbose=2)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "188/188 [==============================] - 15s 53ms/step - loss: 0.1332 - accuracy: 0.9533 - val_loss: 0.3846 - val_accuracy: 0.8463\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 8s 44ms/step - loss: 0.0769 - accuracy: 0.9740 - val_loss: 0.3828 - val_accuracy: 0.8741\n",
            "105/105 - 1s - loss: 0.4277 - accuracy: 0.8569\n",
            "Accuracy: 85.69%\n",
            "Epoch 1/2\n",
            "188/188 [==============================] - 30s 131ms/step - loss: 0.2508 - accuracy: 0.9046 - val_loss: 0.3879 - val_accuracy: 0.8538\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 23s 122ms/step - loss: 0.1706 - accuracy: 0.9386 - val_loss: 0.4372 - val_accuracy: 0.8148\n",
            "105/105 - 3s - loss: 0.4335 - accuracy: 0.8068\n",
            "Accuracy: 80.68%\n",
            "Epoch 1/2\n",
            "188/188 [==============================] - 6s 24ms/step - loss: 0.1646 - accuracy: 0.9392 - val_loss: 0.2656 - val_accuracy: 0.8868\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.0930 - accuracy: 0.9663 - val_loss: 0.3026 - val_accuracy: 0.8846\n",
            "105/105 - 1s - loss: 0.3607 - accuracy: 0.8728\n",
            "Accuracy: 87.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTe6JukVRo6y",
        "outputId": "20feebe0-feda-4983-a5a6-afc0e85ce231"
      },
      "source": [
        "def stacked_prediction(members, x_test, load=False):\n",
        "    yhat = []\n",
        "    for i, m in enumerate(members):\n",
        "        if load:\n",
        "            print(m.predict(x_test, verbose=0))\n",
        "        yhat.append(np.round(m.predict(x_test, verbose=0)))\n",
        "    yhat = np.asarray(yhat)\n",
        "    yhat = [np.round(np.mean(x)) for x in zip(*yhat)]\n",
        "    return np.asarray(yhat).astype('int')\n",
        "\n",
        "yhat = stacked_prediction(members, X_test)\n",
        "acc = accuracy_score(Y_test, yhat)\n",
        "print(\"Accuracy: %.2f%%\" % (acc*100))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 89.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XBJMrmudhBO",
        "outputId": "753e326e-0ea6-4c48-a19b-2f854372a33e"
      },
      "source": [
        "def load_text():\n",
        "    dictionary = imdb.get_word_index()\n",
        "    load_x = []\n",
        "    words = input()\n",
        "    print(words)\n",
        "    words = re.sub(r\"[^a-zA-Z0-9']\", \" \", words)\n",
        "    words = words.split(' ')\n",
        "    valid = []\n",
        "    for word in words:\n",
        "        word = dictionary.get(word)\n",
        "        if word in range(1, 10000):\n",
        "            valid.append(word+3)\n",
        "    load_x.append(valid)\n",
        "    result = sequence.pad_sequences(load_x, maxlen=max_review_length)\n",
        "    print(stacked_prediction(members, result, True))\n",
        "\n",
        "load_text() "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it is a very good movie i really like it good story perfect cast\n",
            "[[0.9719516]]\n",
            "[[0.7488889]]\n",
            "[[0.8125097]]\n",
            "[1]\n"
          ]
        }
      ]
    }
  ]
}